{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_nmt\n",
    "from modified_subproc import SubprocVecEnv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import VecPyTorch\n",
    "import fairseq\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/srikar/pytorch-a2c-ppo-acktr')\n",
    "\n",
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, n_missing_words):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env.init_words(n_missing_words)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "def pad(obs):\n",
    "    obs = list(map(list, zip(*obs)))\n",
    "    source = obs[0]\n",
    "    target = obs[1]\n",
    "    source = sorted(source,key = len,reverse=True)\n",
    "    target = sorted(target,key = len,reverse=True)\n",
    "    m = max(envs.dummyenv.source_lang.len_largest, envs.dummyenv.target_lang.len_largest)\n",
    "    sp = nn.utils.rnn.pad_sequence([torch.ones([m])] + [torch.tensor(s) for s in source] ,batch_first=True)\n",
    "    tp = nn.utils.rnn.pad_sequence([torch.ones([m])] + [torch.tensor(s[:,0]) for s in target] ,batch_first=True)    \n",
    "    \n",
    "    return (sp[1:], tp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 2\n",
    "envs = [make_env(env_id = 'nmt-v0',n_missing_words=2)\n",
    "            for i in range(num_processes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVecEnv(envs)\n",
    "envs = VecPyTorch(envs,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "training_scheme = [1]*10 + [2]*20 + [3]*20\n",
    "\n",
    "base_kwargs={'recurrent': False,'dummyenv':envs.dummyenv,'n_proc':num_processes}\n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,'Attn',base_kwargs)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = algo.PPO(actor_critic, 0.2, 4, 1,\n",
    "                         0.5, 0.001, lr=int(7e-4),\n",
    "                               eps=int(1e-5),\n",
    "                               max_grad_norm=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "n_epochs = 20\n",
    "use_gae = False\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "EOS_token = 1\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/srikar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor actor_features are tensor([[66246.],\n",
      "        [66246.],\n",
      "        [66246.],\n",
      "        [66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[ 66246.],\n",
      "        [164950.],\n",
      "        [164950.],\n",
      "        [ 66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[164950.],\n",
      "        [ 66246.],\n",
      "        [ 66246.],\n",
      "        [164950.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[ 66246.],\n",
      "        [164950.],\n",
      "        [164950.],\n",
      "        [ 66246.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actor actor_features are tensor([[ 66246.],\n",
      "        [ 66246.],\n",
      "        [164950.],\n",
      "        [164950.]], device='cuda:0')\n",
      "value is tensor([[0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071],\n",
      "        [0.5071]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.08 GiB (GPU 0; 5.93 GiB total capacity; 1.30 GiB already allocated; 2.43 GiB free; 962.34 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-da7e7b2832d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     79\u001b[0m \t\t\t\tvalues, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n\u001b[1;32m     80\u001b[0m                     \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_hidden_states_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     masks_batch, actions_batch)\n\u001b[0m\u001b[1;32m     82\u001b[0m                                 \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_log_probs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_action_log_probs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                 \u001b[0msurr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madv_targ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/model.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, inputs, rnn_hxs, masks, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actor actor_features are'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactor_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, rnn_hxs, masks)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mdec_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/models/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prev_output_tokens, encoder_out_dict, incremental_state)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.08 GiB (GPU 0; 5.93 GiB total capacity; 1.30 GiB already allocated; 2.43 GiB free; 962.34 MiB cached)"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    \n",
    "    n_missing_words = training_scheme[epoch]\n",
    "    rollouts = RolloutStorage(num_steps*2*n_missing_words, num_processes,\n",
    "                        envs.observation_space.shape, envs.action_space,\n",
    "                        actor_critic.recurrent_hidden_state_size)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        obs = envs.reset()\n",
    "        rollouts.obs[0].copy_(torch.stack(obs).permute(1,0,2))\n",
    "        \n",
    "        for n in range(2*n_missing_words+1):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                                rollouts.obs[step],\n",
    "                                rollouts.recurrent_hidden_states[step],\n",
    "                                rollouts.masks[step])\n",
    "            \n",
    "            if (n == 2*n_missing_words):\n",
    "                action = torch.zeros([num_processes,1])\n",
    "            obs, reward, done, infos = envs.step(action)\n",
    "            obs = pad(obs)\n",
    "#             masks = torch.FloatTensor([[0.0] if done else [1.0]])\n",
    "\n",
    "            rollouts.insert(torch.stack(obs).permute(1,0,2), recurrent_hidden_states, action, action_log_prob, value, torch.tensor(reward), rollouts.masks[0])\n",
    "\n",
    "        \n",
    "    next_value = 0 #Doubtful\n",
    "\n",
    "    rollouts.compute_returns(next_value, use_gae, gamma, tau)\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = fairseq.models.lstm.LSTMEncoder(envs.dummyenv.source_lang.dict,left_pad=False ,dropout_in=0.0, dropout_out=0.0).cuda()\n",
    "decoder = fairseq.models.lstm.LSTMDecoder(envs.dummyenv.target_lang.dict,dropout_in=0.0, dropout_out=0.0).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = obs[0].long().to(device)\n",
    "t = obs[1].long().to(device) \n",
    "\n",
    "m = max(envs.dummyenv.source_lang.len_largest, envs.dummyenv.target_lang.len_largest)\n",
    "enc_out = encoder(s,(torch.ones([s.shape[0]])*m).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "dec_out = decoder(t,enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_out[0].shape\n",
    "outs = torch.ones([dec_out[0].shape[0],dec_out[0].shape[-1]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(t.shape[0]):\n",
    "    l = (t[i] == 0).nonzero()[0]\n",
    "    outs[i] = dec_out[0][i][l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 207321])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3171, -0.3974, -0.2102,  ...,  0.2822, -0.3874,  0.1375],\n",
       "        [-0.1690, -0.2897, -0.2990,  ...,  0.3168, -0.4605,  0.2344]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srikar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Softmax()\n",
    "sm = m(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.distributions.Categorical(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[154777],\n",
       "        [ 28440]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Categorical(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
