{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import gym\n",
    "import gym_nmt\n",
    "from modified_subproc import SubprocVecEnv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import VecPyTorch\n",
    "import fairseq\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/srikar/pytorch-a2c-ppo-acktr')\n",
    "\n",
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "def make_env(env_id, n_missing_words):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env.init_words(n_missing_words)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "def reshape_all(obs):\n",
    "    max_len = 0\n",
    "    for pair in obs:\n",
    "        s,t = pair\n",
    "        max_len = max(max_len,s.shape[1],t.shape[1])\n",
    "    bigs = []\n",
    "    bigt = []\n",
    "    for pair in obs:\n",
    "        s,t = pair\n",
    "        news = torch.zeros([s.shape[0],max_len])\n",
    "        newt = torch.zeros([t.shape[0],max_len])\n",
    "        news[:,:s.shape[1]] = s\n",
    "        newt[:,:t.shape[1]] = t\n",
    "        bigs.append(news)\n",
    "        bigt.append(newt)\n",
    "    return (bigs,bigt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 3\n",
    "envs = [make_env(env_id = 'nmt-v0',n_missing_words=1)\n",
    "            for i in range(num_processes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVecEnv(envs)\n",
    "envs = VecPyTorch(envs,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "training_scheme = [1]*10 + [2]*20 + [3]*20\n",
    "\n",
    "base_kwargs={'recurrent': False,'dummyenv':envs.dummyenv,'n_proc':num_processes}\n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,'Attn',base_kwargs)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = algo.PPO(actor_critic, 0.2, 4, 4,\n",
    "                         0.5, 0.001, lr=int(7e-4),\n",
    "                               eps=int(1e-5),\n",
    "                               max_grad_norm=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "n_epochs = 20\n",
    "use_gae = False\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "EOS_token = 1\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/srikar/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/model.py:323: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sm = m(outs)\n",
      "/home/srikar/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec outputs are tensor([[-0.1860, -0.1164,  0.1741,  ..., -0.1470,  0.4323, -0.0704]],\n",
      "       device='cuda:0')\n",
      "dec outputs are tensor([[-0.1958, -0.2031,  0.1995,  ..., -0.1798,  0.5217, -0.0494]],\n",
      "       device='cuda:0')\n",
      "dec outputs are tensor([[-0.1312, -0.1915,  0.1014,  ..., -0.1678,  0.4153, -0.0536]],\n",
      "       device='cuda:0')\n",
      "torch.Size([4, 15])\n",
      "torch.Size([4, 15])\n",
      "dec outputs are tensor([[-0.2304, -0.1673,  0.1918,  ..., -0.1259,  0.3032,  0.0867]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 405.00 MiB (GPU 0; 5.93 GiB total capacity; 4.20 GiB already allocated; 61.56 MiB free; 225.28 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b18fffbb8b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m    101\u001b[0m \t\t\t\tnn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n\u001b[1;32m    102\u001b[0m \t\t\t\t\t\t\t\t\t\t self.max_grad_norm)\n\u001b[0;32m--> 103\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                                 \u001b[0mvalue_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 405.00 MiB (GPU 0; 5.93 GiB total capacity; 4.20 GiB already allocated; 61.56 MiB free; 225.28 MiB cached)"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs+1):\n",
    "    \n",
    "    n_missing_words = training_scheme[epoch]\n",
    "    rollouts = RolloutStorage(num_steps*2*(n_missing_words+1)+1, num_processes,\n",
    "                        envs.observation_space.shape, envs.action_space)\n",
    "    \n",
    "    obs = []\n",
    "    masks = []\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        ob = envs.reset()\n",
    "        obs.append(ob)\n",
    "#         obs = reshape_all()\n",
    "        \n",
    "        \n",
    "#         rollouts.obs[0].copy_(torch.stack(obs).permute(1,0,2))\n",
    "        \n",
    "        for n in range(2*n_missing_words+1):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob = actor_critic.act(ob)\n",
    "            \n",
    "            if (n == 2*n_missing_words):\n",
    "                action = (torch.ones([num_processes,1])*2).cuda()\n",
    "            ob, reward, done, infos = envs.step(action)\n",
    "            obs.append(ob)\n",
    "#           mask = torch.FloatTensor([[0.0] if done else [1.0]])\n",
    "    ob1 = obs\n",
    "    obs = reshape_all(obs)\n",
    "    rollouts.insert(obs, action, action_log_prob, value, torch.tensor(reward))\n",
    "\n",
    "        \n",
    "    next_value = 0 #Doubtful\n",
    "\n",
    "    rollouts.compute_returns(next_value, use_gae, gamma, tau)\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "\n",
    "    rollouts.after_update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
